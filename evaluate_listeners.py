
"""
This script evaluates the performance of different Speaker models on the RefOI dataset by using a Vision-Language Model (VLM) as Listener for referring expression comprehension (REC).

- The script takes as input the RefOI dataset, with images and referring descriptions generated by various models (Speakers) or humans.
- The Listener model receives both the image and the description and predicts the bounding box for the referred object.
- Results are evaluated by Intersection over Union (IoU): if IoU > 0.5, the prediction is considered correct.
- Incorrect cases are saved as images for qualitative analysis.

**How to change the Listener model:**
By default, the script loads the Qwen2.5-VL-7B-Instruct model as Listener, but you can easily swap in any other Hugging Face Vision-Language Model.
➡️ To use a different model, **just change the value of `model_name` in the `main()` function** (e.g., to "Qwen/Qwen1.5-VL", "THUDM/cogvlm-grounding-generalist", etc.)
Make sure to also load the correct processor if required by the model you choose.

Example:
    model_name = "Qwen/Qwen2.5-VL-7B-Instruct"
    # Change above line to another model checkpoint for a different VLM Listener

All other parts of the evaluation pipeline (prompting, accuracy computation, visualization of errors) remain unchanged.
"""

#!/usr/bin/env python3
import os
import re
import json
import torch
import warnings
from collections import defaultdict
from PIL import Image, ImageDraw
from datasets import load_dataset
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    Qwen2_5_VLProcessor,
)

warnings.filterwarnings("ignore", category=UserWarning)

# Compute Intersection over Union (IoU) between two bounding boxes
def compute_iou(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interW = max(0, xB - xA)
    interH = max(0, yB - yA)
    interArea = interW * interH
    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    return interArea / (areaA + areaB - interArea + 1e-8)

# Evaluate a Listener model on the dataset for specific Speakers (sources)
def evaluate_per_source(ds, model, processor, total=None, filter_sources=None):
    # Make filter case-insensitive
    if isinstance(filter_sources, str):
        filter_sources = {filter_sources.lower()}
    elif filter_sources is not None:
        filter_sources = {s.lower() for s in filter_sources}

    results = defaultdict(list)           # per-source accuracy
    prompt_results = defaultdict(lambda: defaultdict(list)) # per-source & prompt-type

    total_brief = 0
    total_default = 0
    correct_brief = 0
    correct_default = 0

    base_dir = "wrong_predictions"
    os.makedirs(base_dir, exist_ok=True)
    iterable = ds if total is None else ds.select(range(total))

    for idx, ex in enumerate(iterable):
        raw_source = ex.get("source", "unknown_model")
        source = raw_source.lower()

        # Skip unwanted sources
        if filter_sources and source not in filter_sources:
            continue

        img = ex["image"]
        if isinstance(img, str):
            img = Image.open(img).convert("RGB")

        prompt_type = ex.get("note", ex.get("description_type", "unknown")).lower()
        desc = ex.get("description", ex.get("full_description", ""))

        W, H = img.size
        gold_norm = [ex["box_xmin"], ex["box_ymin"], ex["box_xmax"], ex["box_ymax"]]
        gold_px = [gold_norm[0]*W, gold_norm[1]*H, gold_norm[2]*W, gold_norm[3]*H]

        # Prepare input prompt
        prompt = (
            'Please output a JSON object with key "bbox_2d" and value [x1, y1, x2, y2]'
            f" for the description: {desc}"
        )
        chat_text = processor.apply_chat_template(
            [{"role": "user",
              "content": [{"type": "image", "image": img},
                          {"type": "text",  "text": prompt}]}],
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = processor(text=[chat_text], images=[img],
                           padding=True, return_tensors="pt").to(device)

        # Generate prediction
        out_ids = model.generate(**inputs, max_new_tokens=64,
                                 do_sample=True, temperature=0.7, top_p=0.9)
        raw = processor.batch_decode(out_ids, skip_special_tokens=True,
                                     clean_up_tokenization_spaces=True)[0]

        # Extract predicted bbox
        m = re.search(r'\{.*?"bbox_2d".*?\}', raw, flags=re.DOTALL)
        answer = m.group(0) if m else raw.strip()

        pred_px = [0, 0, 0, 0]
        try:
            obj = json.loads(answer)
            coords = obj.get("bbox_2d", [])
            if len(coords) == 4:
                pred_px = ([coords[0]*W, coords[1]*H, coords[2]*W, coords[3]*H]
                           if all(0.0 <= c <= 1.0 for c in coords) else coords)
        except json.JSONDecodeError:
            mm = re.search(r"\[([\d.,\s]+)\]", answer)
            if mm:
                nums = [float(x) for x in mm.group(1).split(",")]
                pred_px = [nums[0]*W, nums[1]*H, nums[2]*W, nums[3]*H]

        iou = compute_iou(pred_px, gold_px)
        is_correct = iou > 0.5
        results[source].append(is_correct)
        prompt_results[source][prompt_type].append(is_correct)

        # Track stats by prompt type
        if prompt_type == "brief_prompt":
            total_brief += 1
            if is_correct:
                correct_brief += 1
        elif prompt_type == "default_prompt":
            total_default += 1
            if is_correct:
                correct_default += 1

        print(
            f"{raw_source} | {prompt_type} | Desc: {desc}\n"
            f"Image {idx+1}: IoU = {iou:.3f} → {'Correct' if is_correct else 'Wrong'}\n"
        )

        # Save error images for inspection
        if not is_correct:
            sub_dir = os.path.join(base_dir, source.replace(" ", "_"))
            os.makedirs(sub_dir, exist_ok=True)
            filename = f"wrong_{source}_{idx+1}.jpg"
            path = os.path.join(sub_dir, filename)
            draw_img = img.copy()
            draw = ImageDraw.Draw(draw_img)
            draw.rectangle(gold_px, outline="green", width=3)  # Ground truth
            draw.rectangle(pred_px, outline="red", width=3)    # Prediction
            draw_img.save(path)
            print(f"Saved: {path}")

    # Print accuracy per source
    print("\n=== Summary per Source ===")
    for src, outcomes in results.items():
        correct = sum(outcomes)
        total_cases = len(outcomes)
        pct = correct / total_cases * 100
        print(f"{src} {correct}/{total_cases} accurate → {pct:.1f}%")

    # Print breakdown by prompt style
    print("\n=== PROMPT-STYLE ACCURACY COUNTS ===")
    for target in (filter_sources or prompt_results.keys()):
        if target in prompt_results:
            overall_correct = sum(results[target])
            default_correct = sum(prompt_results[target].get("default_prompt", []))
            brief_correct   = sum(prompt_results[target].get("brief_prompt",   []))

            print(f"{target} default correct : {default_correct}")
            print(f"{target} brief   correct : {brief_correct}")
            print(f"{target} total   correct : {overall_correct}\n")

    # Global prompt-type stats
    print("\n=== OVERALL PROMPT-TYPE SUMMARY ===")
    grand_total = total_brief + total_default
    grand_correct = correct_brief + correct_default

    def safe_pct(num, denom):
        return 0.0 if denom == 0 else (num / denom * 100)

    print(f"Number of brief prompts       : {total_brief}")
    print(f"Number of default prompts     : {total_default}")
    print(f"Correct in brief prompts      : {correct_brief}/{total_brief} ({safe_pct(correct_brief, total_brief):.1f}%)")
    print(f"Correct in default prompts    : {correct_default}/{total_default} ({safe_pct(correct_default, total_default):.1f}%)")
    print(f"Correct overall               : {grand_correct}/{grand_total} ({safe_pct(grand_correct, grand_total):.1f}%)")

# Main entry point
def main():
    global device
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    model_name = "Qwen/Qwen2.5-VL-7B-Instruct"  # Change to use a different listener

    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        trust_remote_code=True,
    ).eval().to(device)

    processor = Qwen2_5_VLProcessor.from_pretrained(
        model_name,
        trust_remote_code=True,
    )

    ds = load_dataset("Seed42Lab/RefOI-TLHF", split="train")

    # Choose which Speakers to evaluate
    evaluate_per_source(
        ds,
        model,
        processor,
        total=None,                    # No limit: use all samples
        filter_sources=["llava_34b",  "minicpmv"]
    )

if __name__ == "__main__":
    main()
